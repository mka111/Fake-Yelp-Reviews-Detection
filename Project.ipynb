{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##          The classification of \"fake\" Yelp reviews using Multinomial Naive Bayes Theorem combined with vetting functions\n",
    "Sunaina Paudel, Manpreet Kaur, Isura Kumarapeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "The team chose to explore the Yelp dataset reviews, motivated by the common issues users face with many deceptions found on the site. Through some research, we found that deceptive language consists of distinct signs. These signs consist of various things like types of words used, user verfication, timestamp and percentage of verbs. By using the Yelp dataset provided for the project, we believe we could improve on the impact a sentiment analysis model could have to extract fake reviews if it was combined additionally with functions that would vet out reviews for different signs. The model and functions will help us create a classification system for extreme positive and negative reviews. Our goal is to help users detect all positive and negative reviews that do not portray how the user truly felt about the business. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Goals\n",
    "The goal of this experimentation will be to extract reviews that derive incorrect predictions from a sentiment polarity model and run them thru algorithms that can vet those set of reviews for other signs of \"fake\" sentiment. With the results of this process, we should be able to narrow down the reviews for a business to weed out \"fake\" reviews. \n",
    "\n",
    "Our experiment expands on a reasearch paper[1] which focuses on the detection of fake online hotel reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background \n",
    "\n",
    "The term \"fake\" in this report refers to sentences or paragraphs that portray insincere sentiment in their yelp reviews. This includes bot-written reviews, paid reviews, or insincere reviews written by friends or family of business owners.\n",
    "\n",
    "Various articles relay similar information when it comes to the detection of fake reviews. The following are some highly probable characteristics of reviews which are likely to be fake:\n",
    "\n",
    "1. Lacks details in the description. There are no concrete words used to describe the food. \n",
    "2. There are more verbs (as well as adverbs, adjective superlatives) used than nouns in the description. Deceivers like to paint the picture rather than talk about the product or the food.\n",
    "3. There are suspicious timestamps: extreme reviews are posted over short time frames one after another. \n",
    "4. They are more likely to \"self-reference\" and overuse the words \"I\", \"me, \"my\" etc.. \n",
    "5. A sentiment polarity based model can help define the real sentiment behind reviews. If a sentiment analysis model predicts a review to be the polar opposite of the real ratings, there is a possibility the model inaccurately predicts it due to the insincere wording used in those particular reviews. It should be noted, however, a false prediction is also possible due to biases in the training data.\n",
    "\n",
    "For a specific quote on an article: \n",
    "\n",
    "An article stated:\n",
    "\"We found that deceptive reviews demonstrate the characteristics of imaginative writing, i.e., frequent usage of verbs and adverbs, while truthful reviews demonstrate the characteristics of informative writing, i.e., frequent usage of nouns and adjective (except superlatives, which are more dominant in deceptive reviews due to the exaggerating tendency of deceptive reviewers). After all, the deceptive reviewers must make up a description of a hotel that they never have been to, hence they unconsciously rely on the imaginative writing style.\n",
    "Another unexpected finding is that fake reviewers tend to overdo “self-referencing”, that is, they overuse words such as “I”, “me”, “my”, “mime”, as if they try to underline their existence and credibility.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "###### Introduction of methods\n",
    "\n",
    "The research paper[1] that this experiment is based off gives the accuracy for detecting fake reviews for a hotel to be at 85.9% for multinomial Naive Bayes, with Bermoulli Native Bayes and Logistic Regression slightly behind it. Due to this, we will be using the multinomial NB model and adding vetting functionalities such that the prediction for the list of possible fake reviews will be improved.  \n",
    "\n",
    "##### Method\n",
    "The multinomial NB algorithm will help provide a sentiment polarity based model. It will use \"extreme\" reviews of rating 1 and rating 5 from the training set to define predicted ratings for the test set. The predicted rating will help define if a review is a real positive, real negative or displays opposite ratings for prediction and real. \n",
    "\n",
    "To begin sentiment classification, we must start with a feature vector. For this report, we will use Scikit-learn's algorithm. We pre-process the text into a bag-of-words and remove all stopwords from our list before this step. \n",
    "\n",
    "1. **Scikit-learn's** algorithm is used to convert each review into a vector. The CountVectorizer function analyzes a list given with all the unique words in a review and then converts it into a matrix of token counts. The following matrix shows how many instances of particular word  appear in a review:\n",
    " \n",
    " <img src=\"table.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "2. **Mutinomial Naive Bayes** algorithm is used to build a model and fit it with the training set. After the model has been trained, the predict function predicts the positive and negative review based on the text. The multinomial NB is a type of Naive Bayes that assume independence for all the words in review.\n",
    "The accuracy of our models' prediction of user ratings is 93%.\n",
    " \n",
    " <img src=\"accuracy.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "The naive Bayes is a model that assumes independence for each of its' features. It defines the probability of observing features f1 to fn, given some class c. It our case, the class will be the ratings/stars. \n",
    "\n",
    "Formula:\n",
    "\n",
    "**p(f1,...,fn|c)= ∏(i=1..n) p(fi|c)**\n",
    "\n",
    "\n",
    "The vetting function we define: \n",
    "1. We extract all reviews that have a high number of pronouns.\n",
    "2. We extract all reviews that have a much higher percentage of verbs than nouns. \n",
    "3. We extract all reviews lacking in description.\n",
    "\n",
    "We work with the reviews defined from the above three and compare it to the reviews retrived from the error of the sentiment analysis model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The following cell contains datasets which are subsets of the given yelp data. They consist data for multiple businesses and reviews for those particular business. \n",
    "\n",
    "##### Begin with\n",
    "The information we extract from the business set is: business_id and business stars. \n",
    "The information we extract from the review set is: review_id,user_id, business_id, stars, and text.\n",
    "\n",
    "We extract a total of 20 000 reviews for our model, split into 70% training and 30% test set. We also get 1000 reviews per business to use the model on.\n",
    "\n",
    "##### End with\n",
    "By the end of our code, we collected 5 types of dataframes that will help us list possible fakes by finding the reviews which intersect in those dataframes.\n",
    "1. The list of extremely short reviews with extreme ratings from users.\n",
    "2. The list of reviews that have ratings outside of the average business ratings + threshold. \n",
    "3. The list of reviews which contains verbs at a significantly higer percentage than nouns.\n",
    "4. The list of reviews with a very high percentage of pronouns. \n",
    "5. The list of reviews created from the sentiment analysis model, that extracts reviews with have oppositite ratings from the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "The implementation of Scikit-learn's algorithm and Naive Bayes algorithm was shaped from open source. The code had to be modified by the team to work with our dataframes. The rest of the functions were made by the team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Setup\n",
    "\n",
    "###### our collected information\n",
    "By the end of our code, we have collected 5 types of dataframes that will help us list possible fakes by finding the reviews which intersect in those dataframes.\n",
    "1. The list of extremely short reviews with extreme ratings from users.\n",
    "2. The list of reviews that have ratings outside of the average business ratings + threshold. \n",
    "3. The list of reviews which contains verbs at a significantly higer percentage than nouns.\n",
    "4. The list of reviews with a very high percentage of pronouns. \n",
    "5. The list of reviews created from the sentiment analysis model, that extracts reviews with have oppositite ratings from the prediction. \n",
    "\n",
    "###### Our Analysis\n",
    "We begin to analyze results by vetting the possible fakes returned from the multinomialNB function. We extract all review where it has significantly higher verbs than nouns, high percentage of pronouns and all short reviews with extreme ratings. \n",
    "\n",
    "We will count the avergage number of fakes the multinomialNB and each comparison will return. We will define how much the list of fake reviews is narrowed down.\n",
    "\n",
    "The results are will be flagged into the following categories of probability for a fake review after getting the results:\n",
    "\n",
    "HIGH PROBABILITY: \n",
    "1.\tIf review displays fake sentiment and has a significantly higher percentage of verbs than nouns. \n",
    "\n",
    "INTERMEDIATE PROBABILITY:\n",
    "1.\tIf the review is outside threshold and has a significantly higher percentage of verbs than nouns. \n",
    "2.\tIf the review has high percentage of both verbs and pronouns. \n",
    "3.\tIf the review displays fake sentiment and has a significantly higher percentage of pronouns. \n",
    "\n",
    "LOW PROBABILITY:\n",
    "1.\tIf the review is outside of threshold and has high percentage of pronouns\n",
    "2.\tIf the review is outside of threshold and is too short \n",
    "\n",
    "\n",
    "Ideally, we are trying to compare our result to the accuracy returned by research paper[1], hoping that the added vetting function will improve on the detection of fake reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "\n",
    "Unfortunately, we are unable to compare the accuracy of the results due to our lack of fake data. This mean we cannot compare our method to the baseline. Our only possible evaluation will be the comparisons of the number of outputs the MultinomialNB will return when processed with the vetting functions. The only accuracy evaluation we were able to do was human detection for the returned list of outputs.\n",
    "\n",
    "On average, we retrived 120 possible fake reviews from the multinomial model. Out of the 120, on average we found:\n",
    "- approx. 10% were flagged as fake due to high pronouns.\n",
    "- approx. 30% were flagged due to high percentage of verbs. \n",
    "\n",
    "We also found that a few reviews, which had ratings outside of the given business threshold, returned on average 13 reviews that were marked with high percentage of verbs and pronouns. The reviews were missed by the multinomial model. \n",
    "\n",
    "This is possible that due to biases, the multinomialNB falsely predicted the some number of reviews. This may have caused some fake reviews to be passed by the model which were then possibly caught by reviews marked as \"outsdie_of_threshold\". \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work \n",
    "We would like to analyze the user data, in relation to the reviews, and incorporate it into the vetting functions. One of our possible vetting function would be to check if a user posts too many reviews at the same time; it is significantly likely to be a fake if this is true.\n",
    "\n",
    "We would also create some fake data to be incorporated into the set of reviews to test with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run yelpf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_Path = \"data/yelp_academic_dataset_business.json\" \n",
    "rev_Path = \"data/yelp_academic_dataset_review.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following documentations refer to the cells of code directly.\n",
    "We begin with some of our simple functions used to mark possible fake reviews.\n",
    "\n",
    "The following lines of code take in the review files in json format and create a dataframe to work with. The number of reviews and the particular businesses they are for can be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###return list of business\n",
    "testBus=find_Business(bus_Path,11,1000)\n",
    "Bus_One = testBus[7]\n",
    "print(Bus_One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###returns reviews\n",
    "testRev=review_list(rev_Path,1000,Bus_One)\n",
    "print(\"Review: \",testRev, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the reviews collected as a new dataframe and adds 'text length'\n",
    "test_dataframe = import_to_dataframe(testRev)\n",
    "print(test_dataframe)\n",
    "l = len(test_dataframe)\n",
    "print(\"l \",l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell consists of a function which detects the reviews with ratings far outside of the average. The assumption we make is that the sentiment analysis model we use will ignore some possible fake review that could be caught by other simpler methods. \n",
    "\n",
    "We give each business a threshold for the ratings outside of the average. We chose the threshold to be 2. We proceed to flag any ratings outside of the threshold as possible fakes that need to be examined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : outside a thresh\n",
    "#reviews ratings/star is outside a defined threshold\n",
    "def rev_outside_thres(bus_rating,rev_list,tresh):\n",
    "    positiveTresh = bus_rating+tresh\n",
    "    negativeTresh = bus_rating-tresh\n",
    "    y = test_dataframe[(test_dataframe['stars'] < negativeTresh) | (test_dataframe['stars'] > positiveTresh)]\n",
    "    return y\n",
    "\n",
    "#has the same list struct as review_list but only contains possible fake reviews\n",
    "OutsideTresh=rev_outside_thres(Bus_One[1],testRev,2)\n",
    "print(OutsideTresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below flags short reviews with extreme ratings as possible fakes to be examined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get business avg rating given business id\n",
    "def get_bus_rating(bid,busList):\n",
    "    for busId in busList:\n",
    "        if busId[0]==bid:\n",
    "            return busId[1]\n",
    "    return 0\n",
    "#print(get_bus_rating(\"5T6kFKFycym_GkhgOiysIw\",testBus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 : short \n",
    "# only return results with big difference from avergae value\n",
    "\n",
    "\n",
    "\n",
    "avg_ratings = get_bus_rating(test_dataframe['business_id'][0],testBus)\n",
    "x = rev_outside_thres(avg_ratings,testBus,0)\n",
    "#print(x)\n",
    "print(avg_ratings)\n",
    "ShortReviews = x[x['text length'] <= 50]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(ShortReviews)\n",
    "#print(\"Out_of_Thresh: \",testTresh, \"\\n\")\n",
    "#shortTest = short_reviews(testRev, 30)\n",
    "#print(\"ShortReviews_outofTresh: \",shortTest,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell begins the **sentiment analysis model**. \n",
    "\n",
    "The function import2frame_filePath() takes the list of ALL reviews (to a certain limit). This new dataframe created from it will allow us to examine correlations between words in reviews and the user ratings given to those reviews. This will help build the sentiment analysis model; we will be able to use the model to detect the reviews that are predicted to have postive sentiment but display negative ratings (and vise versa). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 : sentiment in reviews\n",
    "#function to create a dataframe from the list of ALL reviews\n",
    "#on the while line, you may change the number of reviews \n",
    "#the higher the number, the better the training model\n",
    "def import2frame_filePath(rev_Path_Path):\n",
    "    lis= []\n",
    "    counter = 0\n",
    "    with open(rev_Path_Path, encoding=\"utf8\") as fp:\n",
    "        line = fp.readline()\n",
    "        while line and counter <2000:\n",
    "            line=fp.readline()\n",
    "            datastore=json.loads(line)\n",
    "            lis.append(datastore)\n",
    "            counter=counter+1\n",
    "    df = pd.DataFrame.from_dict(lis)\n",
    "    df.shape\n",
    "    #print(df.describe())\n",
    "    #print(df)\n",
    "    # a new column to store the character length of each review\n",
    "    df['text length'] = df['text'].apply(len)\n",
    "    stars = df.groupby('stars').mean()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract all the \"extreme\" reviews (with ratings either 1 or 5), and extract all the text and the correlated ratings from those reviews. \n",
    "\n",
    "We build a sparse matrix with words tokenized from the reviews, and with word count correlating the word to each review. This will help us understand the associations between particular words in positive and negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to create training model for sentiment analysis\n",
    "df = import2frame_filePath(rev_Path)\n",
    "#Predict if a review is good or bad, grab reviews that are either 1 or 5 stars from datatframe\n",
    "yelp_class = df[(df['stars'] == 1) | (df['stars'] == 5)]\n",
    "#print(yelp_class.shape)\n",
    "X = yelp_class['text'] #text column of yelp_class\n",
    "y = yelp_class['stars'] #stars column of yelp_class\n",
    "\n",
    "#goal : convert text collection into a matrix of token counts \n",
    "#each row is unique word and each coloumn is a review\n",
    "#there will be lots of zeros in the matrix --> sparse matrix \n",
    "#convert each review into a vector\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(X)\n",
    "length = len(bow_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move on to create the test data and the training data. Use 30% of the data to test. We train the model using multinomial **Naive Bayes** made for text documents, using the function MultinomialNB(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform X dataframe into a sparse matrix. Used .transform() method on our bag of words transformed object\n",
    "X = bow_transformer.transform(X)\n",
    "#print('Shape of Sparse Matrix: ', X.shape)\n",
    "#print('Amount of Non-Zero occurrences: ', X.nnz)\n",
    "\n",
    "# Percentage of non-zero values\n",
    "density = (100.0 * X.nnz / (X.shape[0] * X.shape[1]))\n",
    "#print('Density: {}'.format((density)))\n",
    "\n",
    "#spliting the test data and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "#predicts the ratings of previously unseen reviews(reviews from test set)\n",
    "preds = nb.predict(X_test)\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will define a **bi-gram** model to compare to Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function find_fake_review() will extract all the reviews that the sentiment analysis model did not accurately predict to be true. While these bad predictions could be due to possible bias in the previous model, they could be possible dectection of inconsistencies in the language with positive and negative contexts. This could be a display of fake reviews, and hence, fake sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_from_sentiment(dataframe,counter):\n",
    "    col_lis = list(dataframe.columns.values)\n",
    "    fake_list = pd.DataFrame(columns=col_lis)\n",
    "    i=0\n",
    "    while i <counter:\n",
    "        rev = dataframe['text'][i]\n",
    "        strRev = str(rev)\n",
    "        star = dataframe['stars'][i]\n",
    "        pp = bow_transformer.transform([rev])\n",
    "        pre = nb.predict(pp)[0]\n",
    "        if(pre == 1):\n",
    "            if(star > 3):\n",
    "                #print(\"PREDICT\", pre, \"STAR\", star,\"\\n\")\n",
    "                #print(\"REVIEWWWW\",rev,\"\\n\")\n",
    "                fake_list = fake_list.append(dataframe.iloc[i], ignore_index=True)\n",
    "        if(pre == 5):\n",
    "            if(star < 3):\n",
    "                #print(\"PREDICT\", pre, \"STAR\", star,\"\\n\")\n",
    "                #print(\"REVIEW\",rev,\"\\n\")\n",
    "                fake_list = fake_list.append(dataframe.iloc[i], ignore_index=True)\n",
    "        i = i+1\n",
    "        \n",
    "    print(fake_list)\n",
    "    return fake_list\n",
    "\n",
    "#ans1 = pr_lis(fakelist,m)\n",
    "fakeSentiment = fake_from_sentiment(test_dataframe,l)\n",
    "#print(fakrSentiment['business_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write output to files \n",
    "with open('draft.txt', 'w+') as f:\n",
    "        f.write(ans1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to extract the number of nouns, verbs and pronouns from each review. Through research, we found that reviews with much higher verbs than nouns were more likely to be fake. Reviews with high number of pronouns were also more likely to be fake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4 : number of nouns, verbs and pronouns \n",
    "#IGNORE THE ERROR : it is due to writing over a copy frame in the tag_sentences()\n",
    "#return the dataframe with added percentages of nouns, verbs, and pronouns\n",
    "#l is length of text_dataframe \n",
    "testTag = tag_sentences(test_dataframe, l)\n",
    "print(testTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below we have 2 cases:\n",
    "# 1 - reviews that may be fake because they have too many verbs\n",
    "# 2 - reviews that may be fake because they have too many pronouns\n",
    "test_FakeDue2Verbs = comparisons_n_v(test_dataframe,l)\n",
    "test_FakeDue2Pronouns = comparisons_p(test_dataframe,l)\n",
    "\n",
    "#print(test_FakeDue2Verbs['text'])\n",
    "print(\"High Verbs: \",len(test_FakeDue2Verbs),\"\\n\")\n",
    "#print_s(test_FakeDue2Verbs)\n",
    "print(\"\\n\\nHigh Pronouns : \",len(test_FakeDue2Pronouns),\"\\n\")\n",
    "#print_s(test_FakeDue2Pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fakeSentiment))\n",
    "print(len(test_FakeDue2Verbs))\n",
    "print(len(test_FakeDue2Pronouns))\n",
    "print(len(ShortReviews))\n",
    "print(len(OutsideTresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take all reviews that have very high percentage of verbs and pronouns. We assume that the existence of the reviews in both list derives them to have a higher chance of being a fake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 - intermediate\n",
    "#testing for reviews with high pronouns AND verbs +\n",
    "merged = pd.merge(test_FakeDue2Verbs, test_FakeDue2Pronouns, on=['review_id'], how='inner')\n",
    "#print(merged)\n",
    "get_lis(merged, 'text_x', 'stars_x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 - high\n",
    "#fakelist = fake from sentiment analysis\n",
    "#test_FakeDue2Verbs \n",
    "#print(fakeSentiment,\"\\n\")\n",
    "#print(test_FakeDue2Verbs)\n",
    "\n",
    "#testing for reviews with high pronouns AND verbs +\n",
    "high_fakes = pd.merge(fakeSentiment, test_FakeDue2Verbs, on=['review_id'], how='inner')\n",
    "#print(merged)\n",
    "ans3 = get_lis(merged2, 'text_x', 'stars_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - low \n",
    "#testing for reviews with high pronouns AND verbs +\n",
    "merged3 = pd.merge(fakeSentiment, test_FakeDue2Pronouns, on=['review_id'], how='inner')\n",
    "#print(merged)\n",
    "ans5 = get_lis(merged3, 'text_x', 'stars_x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4 - intermediate \n",
    "#print(OutsideTresh)\n",
    "#testing for reviews with high pronouns AND verbs +\n",
    "\n",
    "merged4 = pd.merge(OutsideTresh, test_FakeDue2Verbs, on=['review_id'], how='inner')\n",
    "#print(merged)\n",
    "ans6 = get_lis(merged4, 'text_x', 'stars_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - low \n",
    "#print(OutsideTresh)\n",
    "#testing for reviews with high pronouns AND verbs +\n",
    "merged5 = pd.merge(OutsideTresh, test_FakeDue2Pronouns, on=['review_id'], how='inner')\n",
    "#print(merged)\n",
    "ans6 = get_lis(merged5, 'text_x', 'stars_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "#### Research Papers: \n",
    "[1] **Detection of fake online hotel reviews**\n",
    "Anna V. Sandifer, Casey Wilson (2017)\n",
    "\n",
    "[2] **A framework for sentiment analysis with opinion mining of hotel reviews**. Kudakwashe Zvarevashe, Oludayo O. Olugbara\n",
    "(2018)\n",
    "\n",
    "[3] **A survey on sentiment analysis methods and approach**.\n",
    "A. M. Abirami, V. Gayathri\n",
    "(2016)\n",
    "\n",
    "\n",
    "#### Online Articles:\n",
    "https://www.inc.com/jessica-stillman/heres-how-to-spot-fake-online-reviews-with-90-perc.html\n",
    "\n",
    "https://think-data.github.io/machine%20learning/python/nlp/2016/12/26/detecting-fake-reviews.html\n",
    "\n",
    "https://medium.com/tensorist/classifying-yelp-reviews-using-nltk-and-scikit-learn-c58e71e962d9\n",
    "\n",
    "https://www.cs.stonybrook.edu/about-us/News/Catching-Fake-Reviews-Linguistic-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
